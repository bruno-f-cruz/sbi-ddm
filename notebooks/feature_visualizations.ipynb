{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Feature Visualizations (37 Features)\n",
    "\n",
    "This notebook visualizes how all 37 summary statistics respond to changes in DDM parameters.\n",
    "\n",
    "**Feature Groups:**\n",
    "- Basic (7): Max, mean, std of times, stops, rewards\n",
    "- Reward History (4): Behavior after reward vs failure\n",
    "- Temporal (5): Early/late session dynamics\n",
    "- Distribution (4): Percentiles and shape\n",
    "- Sequential (3): Autocorrelation and persistence\n",
    "- Reward Stats (3): Detailed reward behavior\n",
    "- Patch Stats (3): Exit patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Force CPU backend on Apple Silicon to avoid Metal issues\n",
    "os.environ['JAX_PLATFORMS'] = 'cpu'\n",
    "\n",
    "# === Setup ===\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "import seaborn as sns\n",
    "\n",
    "from aind_behavior_vrforaging_analysis.sbi_ddm_analysis.simulator import PatchForagingDDM_JAX, create_prior\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# Initialize simulator\n",
    "simulator = PatchForagingDDM_JAX(max_sites_per_window=100)\n",
    "rng_key = random.PRNGKey(42)\n",
    "\n",
    "FEATURE_NAMES = [\n",
    "    # Basic (7)\n",
    "    \"max_time\", \"mean_time\", \"std_time\", \n",
    "    \"mean_stops\", \"std_stops\", \"mean_rewards\", \"std_rewards\",\n",
    "    \n",
    "    # Reward history (4) - KEY FEATURES\n",
    "    \"mean_time_after_reward\",    \n",
    "    \"mean_time_after_failure\",   \n",
    "    \"std_time_after_reward\",\n",
    "    \"std_time_after_failure\",\n",
    "    \n",
    "    # Temporal (5)\n",
    "    \"early_mean\", \"late_mean\", \"temporal_trend\",\n",
    "    \"late_minus_early\", \"middle_mean\",\n",
    "    \n",
    "    # Distribution (4)\n",
    "    \"p25\", \"median\", \"p75\", \"iqr\",\n",
    "    \n",
    "    # Sequential (3)\n",
    "    \"autocorr_lag1\", \"diff_std\", \"mean_abs_change\",\n",
    "    \n",
    "    # Reward stats (3)\n",
    "    \"reward_rate\", \"mean_reward_trial\", \"prop_patches_with_reward\",\n",
    "    \n",
    "    # Patch stats (3)\n",
    "    \"n_patches\", \"mean_sites_per_patch\", \"stop_rate\",\n",
    "    \n",
    "    # Consistency stats (6) - NEW: Distinguish systematic effects from noise\n",
    "    \"after_reward_cv\",              # Coefficient of variation after rewards\n",
    "    \"after_failure_cv\",             # Coefficient of variation after failures\n",
    "    \"transition_reliability\",       # Consistency of reward→failure transitions\n",
    "    \"reward_effect_predictability\", # How well rewards predict next duration\n",
    "    \"mean_local_std\",              # Average local variability\n",
    "    \"signal_to_noise\",             # Between-context / within-context variance\n",
    "    \"failure_effect\",               #Time change from average after a failure\n",
    "    \"reward_effect\"                 #Time change from average after a reward\n",
    "]\n",
    "\n",
    "# Feature groups for organized visualization\n",
    "FEATURE_GROUPS = {\n",
    "    \"Basic\": list(range(0, 7)),\n",
    "    \"Reward History\": list(range(7, 11)),\n",
    "    \"Temporal\": list(range(11, 16)),\n",
    "    \"Distribution\": list(range(16, 20)),\n",
    "    \"Sequential\": list(range(20, 23)),\n",
    "    \"Reward Stats\": list(range(23, 26)),\n",
    "    \"Patch Stats\": list(range(26, 37)),\n",
    "}\n",
    "\n",
    "def simulate_and_extract(theta, rng_key, window_sites=100):\n",
    "    \"\"\"\n",
    "    Runs one simulation for given theta and extracts features.\n",
    "    Returns (window_data, summary_stats, new_rng_key)\n",
    "    \"\"\"\n",
    "    rng_key, subkey = random.split(rng_key)\n",
    "    window_data, summary_stats = simulator.simulate_one_window(theta, subkey)\n",
    "    return window_data, summary_stats, rng_key\n",
    "\n",
    "print(f\"✓ Loaded simulator with {len(FEATURE_NAMES)} features\")\n",
    "print(f\"✓ Feature groups: {list(FEATURE_GROUPS.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Sweep Setup ===\n",
    "theta_labels = [\"drift_rate\", \"reward_bump\", \"failure_bump\", \"noise_std\"]\n",
    "theta_base = jnp.array([0.4, 0.3, 0.1, 0.1])  # Base parameters\n",
    "\n",
    "n_repeats = 20  # Runs per parameter combination\n",
    "gradient_values = np.linspace(0.01, 1.0, 5)  # 5 levels for gradient\n",
    "x_values = np.linspace(0.01, 1.0, 10)  # 10 points for x-axis\n",
    "\n",
    "# Storage for results\n",
    "results_mean = {label: {} for label in theta_labels}\n",
    "results_sem = {label: {} for label in theta_labels}\n",
    "\n",
    "print(f\"Will run {len(theta_labels)} parameter sweeps\")\n",
    "print(f\"Each sweep: {len(gradient_values)} gradients × {len(x_values)} x-values × {n_repeats} repeats\")\n",
    "print(f\"Total simulations: {len(theta_labels) * len(gradient_values) * len(x_values) * n_repeats}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Run Parameter Sweeps ===\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for param_idx, param_x in enumerate(theta_labels):\n",
    "    print(f\"\\nSweeping {param_x} ({param_idx+1}/{len(theta_labels)})...\")\n",
    "    \n",
    "    # Get other parameters (for gradient and fixed)\n",
    "    other_params = [p for j, p in enumerate(theta_labels) if j != param_idx]\n",
    "    gradient_param_idx = theta_labels.index(other_params[0])  # First other param varies\n",
    "    \n",
    "    for grad_val in gradient_values:\n",
    "        mean_list, sem_list = [], []\n",
    "        \n",
    "        for x_val in x_values:\n",
    "            # Build theta for this combination\n",
    "            theta = theta_base.copy()\n",
    "            theta = theta.at[param_idx].set(x_val)  # X-axis parameter\n",
    "            theta = theta.at[gradient_param_idx].set(grad_val)  # Gradient parameter\n",
    "            # Other parameters stay at base values\n",
    "            \n",
    "            # Run multiple simulations\n",
    "            runs = []\n",
    "            for _ in range(n_repeats):\n",
    "                _, summary, rng_key = simulate_and_extract(theta, rng_key)\n",
    "                runs.append(np.array(summary))\n",
    "            \n",
    "            runs = np.vstack(runs)\n",
    "            mean_list.append(runs.mean(axis=0))\n",
    "            sem_list.append(runs.std(axis=0, ddof=1) / np.sqrt(n_repeats))\n",
    "        \n",
    "        # Store results\n",
    "        key = f\"{grad_val:.4f}\"\n",
    "        results_mean[param_x][key] = np.vstack(mean_list)\n",
    "        results_sem[param_x][key] = np.vstack(sem_list)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"  Completed in {elapsed:.1f}s\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n✓ All sweeps completed in {total_time/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 1: Key Features by Parameter\n",
    "\n",
    "Focus on the most important features for each parameter:\n",
    "- **drift_rate**: Basic time statistics, temporal trends\n",
    "- **reward_bump**: Reward history effects (mean_time_after_reward)\n",
    "- **failure_bump**: Reward history effects (mean_time_after_failure)\n",
    "- **noise_std**: Distribution shape, sequential dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Visualization 1: Most Important Features ===\n",
    "\n",
    "# Define key features to visualize for each parameter\n",
    "KEY_FEATURES = {\n",
    "    \"drift_rate\": [1, 2, 11, 12, 13],  # mean_time, std_time, early_mean, late_mean, trend\n",
    "    \"reward_bump\": [7, 8, 9, 14, 23],  # After reward/failure, late-early, reward_rate\n",
    "    \"failure_bump\": [19, 8, 9, 10, 14], # After reward/failure stats\n",
    "    \"noise_std\": [2, 16, 18, 19, 20],  # std_time, p25, p75, iqr, autocorr\n",
    "}\n",
    "\n",
    "for param_x in theta_labels:\n",
    "    other_params = [p for j, p in enumerate(theta_labels) if p != param_x]\n",
    "    gradient_param = other_params[0]\n",
    "    \n",
    "    # Get key features for this parameter\n",
    "    feature_indices = KEY_FEATURES[param_x]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(feature_indices), figsize=(4*len(feature_indices), 4))\n",
    "    if len(feature_indices) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    fig.suptitle(f\"Effect of {param_x} (gradient: {gradient_param})\", fontsize=14, fontweight='bold')\n",
    "    \n",
    "    cmap = plt.cm.viridis(np.linspace(0, 1, len(gradient_values)))\n",
    "    \n",
    "    for ax_idx, feature_idx in enumerate(feature_indices):\n",
    "        ax = axes[ax_idx]\n",
    "        feature_name = FEATURE_NAMES[feature_idx]\n",
    "        \n",
    "        for color_idx, grad_val in enumerate(gradient_values):\n",
    "            key = f\"{grad_val:.4f}\"\n",
    "            mean_vals = results_mean[param_x][key][:, feature_idx]\n",
    "            sem_vals = results_sem[param_x][key][:, feature_idx]\n",
    "            \n",
    "            ax.plot(x_values, mean_vals, color=cmap[color_idx], \n",
    "                   marker='o', markersize=4, linewidth=2,\n",
    "                   label=f\"{grad_val:.2f}\")\n",
    "            ax.fill_between(x_values,\n",
    "                          mean_vals - 1.96 * sem_vals,\n",
    "                          mean_vals + 1.96 * sem_vals,\n",
    "                          color=cmap[color_idx], alpha=0.2)\n",
    "        \n",
    "        ax.set_title(feature_name, fontsize=11, fontweight='bold')\n",
    "        ax.set_xlabel(param_x, fontsize=10)\n",
    "        ax.set_ylabel('Value', fontsize=10)\n",
    "        ax.grid(True, linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # Add legend\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='center left', bbox_to_anchor=(1, 0.5),\n",
    "              title=gradient_param, fontsize=9)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 0.95, 0.96])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 2: Feature Groups\n",
    "\n",
    "Visualize all features organized by functional groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Visualization 2: All Features by Group ===\n",
    "\n",
    "for param_x in theta_labels:\n",
    "    other_params = [p for j, p in enumerate(theta_labels) if p != param_x]\n",
    "    gradient_param = other_params[0]\n",
    "    \n",
    "    # Create one large figure with subplots for each feature group\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    fig.suptitle(f\"All Features: {param_x} (gradient: {gradient_param})\", \n",
    "                fontsize=16, fontweight='bold', y=0.995)\n",
    "    \n",
    "    cmap = plt.cm.viridis(np.linspace(0, 1, len(gradient_values)))\n",
    "    \n",
    "    # Create subplots for each group\n",
    "    group_row = 0\n",
    "    for group_name, feature_indices in FEATURE_GROUPS.items():\n",
    "        n_features = len(feature_indices)\n",
    "        \n",
    "        # Add group title\n",
    "        ax_title = plt.subplot(8, 7, group_row * 7 + 1)\n",
    "        ax_title.text(0.5, 0.5, group_name, fontsize=12, fontweight='bold',\n",
    "                     ha='center', va='center')\n",
    "        ax_title.axis('off')\n",
    "        \n",
    "        # Plot features in this group\n",
    "        for i, feature_idx in enumerate(feature_indices):\n",
    "            ax = plt.subplot(8, 7, group_row * 7 + i + 2)\n",
    "            feature_name = FEATURE_NAMES[feature_idx]\n",
    "            \n",
    "            for color_idx, grad_val in enumerate(gradient_values):\n",
    "                key = f\"{grad_val:.4f}\"\n",
    "                mean_vals = results_mean[param_x][key][:, feature_idx]\n",
    "                sem_vals = results_sem[param_x][key][:, feature_idx]\n",
    "                \n",
    "                ax.plot(x_values, mean_vals, color=cmap[color_idx], \n",
    "                       marker='o', markersize=3, linewidth=1.5, alpha=0.8)\n",
    "                ax.fill_between(x_values,\n",
    "                              mean_vals - 1.96 * sem_vals,\n",
    "                              mean_vals + 1.96 * sem_vals,\n",
    "                              color=cmap[color_idx], alpha=0.15)\n",
    "            \n",
    "            ax.set_title(feature_name, fontsize=8)\n",
    "            ax.tick_params(labelsize=7)\n",
    "            ax.grid(True, linestyle='--', alpha=0.3)\n",
    "        \n",
    "        group_row += 1\n",
    "    \n",
    "    # Add legend\n",
    "    legend_ax = plt.subplot(7, 7, 49)\n",
    "    for color_idx, grad_val in enumerate(gradient_values):\n",
    "        legend_ax.plot([], [], color=cmap[color_idx], linewidth=3, \n",
    "                      label=f\"{grad_val:.2f}\")\n",
    "    legend_ax.legend(title=gradient_param, loc='center', fontsize=8)\n",
    "    legend_ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.99])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 3: Reward History Effects\n",
    "\n",
    "Deep dive into the critical reward history features that distinguish reward_bump from failure_bump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Visualization 3: Reward History Deep Dive ===\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Reward History Effects: Critical Features for Bump Parameters', \n",
    "            fontsize=14, fontweight='bold')\n",
    "\n",
    "# Feature indices for reward history\n",
    "reward_features = [7, 8, 9, 10]  # After reward mean/std, after failure mean/std\n",
    "params_to_show = ['reward_bump', 'failure_bump']\n",
    "\n",
    "for param_idx, param_x in enumerate(params_to_show):\n",
    "    other_params = [p for j, p in enumerate(theta_labels) if p != param_x]\n",
    "    gradient_param = other_params[0]\n",
    "    \n",
    "    cmap = plt.cm.viridis(np.linspace(0, 1, len(gradient_values)))\n",
    "    \n",
    "    for feat_idx, feature_idx in enumerate(reward_features):\n",
    "        ax = axes[param_idx, feat_idx]\n",
    "        feature_name = FEATURE_NAMES[feature_idx]\n",
    "        \n",
    "        for color_idx, grad_val in enumerate(gradient_values):\n",
    "            key = f\"{grad_val:.4f}\"\n",
    "            mean_vals = results_mean[param_x][key][:, feature_idx]\n",
    "            sem_vals = results_sem[param_x][key][:, feature_idx]\n",
    "            \n",
    "            ax.plot(x_values, mean_vals, color=cmap[color_idx], \n",
    "                   marker='o', markersize=5, linewidth=2.5,\n",
    "                   label=f\"{gradient_param}={grad_val:.2f}\")\n",
    "            ax.fill_between(x_values,\n",
    "                          mean_vals - 1.96 * sem_vals,\n",
    "                          mean_vals + 1.96 * sem_vals,\n",
    "                          color=cmap[color_idx], alpha=0.2)\n",
    "        \n",
    "        ax.set_title(f\"{param_x} → {feature_name}\", fontsize=11, fontweight='bold')\n",
    "        ax.set_xlabel(param_x, fontsize=10)\n",
    "        ax.set_ylabel('Time (s)', fontsize=10)\n",
    "        ax.grid(True, linestyle='--', alpha=0.3)\n",
    "        \n",
    "        if feat_idx == 0:\n",
    "            ax.legend(fontsize=8, loc='best')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insight:\")\n",
    "print(\"  • reward_bump should primarily affect 'mean_time_after_reward' (feature 7)\")\n",
    "print(\"  • failure_bump should primarily affect 'mean_time_after_failure' (feature 8)\")\n",
    "print(\"  • These features enable parameter identifiability!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 4: Feature Sensitivity Heatmap\n",
    "\n",
    "Show which features are most sensitive to each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Visualization 4: Sensitivity Heatmap ===\n",
    "\n",
    "# Compute sensitivity as the range (max - min) of each feature\n",
    "# across parameter values (using middle gradient value)\n",
    "sensitivity_matrix = np.zeros((len(theta_labels), len(FEATURE_NAMES)))\n",
    "\n",
    "middle_gradient_idx = len(gradient_values) // 2\n",
    "middle_gradient_val = gradient_values[middle_gradient_idx]\n",
    "key = f\"{middle_gradient_val:.4f}\"\n",
    "\n",
    "for param_idx, param_x in enumerate(theta_labels):\n",
    "    means = results_mean[param_x][key]\n",
    "    for feat_idx in range(len(FEATURE_NAMES)):\n",
    "        # Compute normalized range\n",
    "        feat_values = means[:, feat_idx]\n",
    "        feat_range = np.max(feat_values) - np.min(feat_values)\n",
    "        feat_mean = np.mean(feat_values)\n",
    "        # Normalize by mean to get relative sensitivity\n",
    "        sensitivity_matrix[param_idx, feat_idx] = feat_range / (np.abs(feat_mean) + 1e-6)\n",
    "\n",
    "# Plot heatmap\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "im = ax.imshow(sensitivity_matrix, aspect='auto', cmap='YlOrRd', interpolation='nearest')\n",
    "\n",
    "# Set ticks and labels\n",
    "ax.set_xticks(np.arange(len(FEATURE_NAMES)))\n",
    "ax.set_yticks(np.arange(len(theta_labels)))\n",
    "ax.set_xticklabels(FEATURE_NAMES, rotation=90, fontsize=8)\n",
    "ax.set_yticklabels(theta_labels, fontsize=10)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label('Relative Sensitivity (Range/Mean)', rotation=270, labelpad=20)\n",
    "\n",
    "# Add title\n",
    "ax.set_title('Feature Sensitivity to Parameters (Higher = More Informative)', \n",
    "            fontsize=12, fontweight='bold', pad=20)\n",
    "\n",
    "# Add grid\n",
    "ax.set_xticks(np.arange(len(FEATURE_NAMES)) - 0.5, minor=True)\n",
    "ax.set_yticks(np.arange(len(theta_labels)) - 0.5, minor=True)\n",
    "ax.grid(which='minor', color='white', linestyle='-', linewidth=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print top features for each parameter\n",
    "print(\"\\nTop 5 Most Sensitive Features per Parameter:\")\n",
    "print(\"=\"*70)\n",
    "for param_idx, param_x in enumerate(theta_labels):\n",
    "    top_indices = np.argsort(sensitivity_matrix[param_idx])[-5:][::-1]\n",
    "    print(f\"\\n{param_x}:\")\n",
    "    for rank, idx in enumerate(top_indices, 1):\n",
    "        sens = sensitivity_matrix[param_idx, idx]\n",
    "        print(f\"  {rank}. {FEATURE_NAMES[idx]:30s} (sensitivity: {sens:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 5: Pairwise Feature Relationships\n",
    "\n",
    "Examine correlations between key features to understand redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Visualization 5: Feature Correlation Analysis ===\n",
    "\n",
    "# Collect all feature values across all simulations\n",
    "all_features = []\n",
    "\n",
    "for param_x in theta_labels:\n",
    "    for key in results_mean[param_x].keys():\n",
    "        # Get all x-values for this gradient level\n",
    "        all_features.append(results_mean[param_x][key])\n",
    "\n",
    "all_features = np.vstack(all_features)\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = np.corrcoef(all_features.T)\n",
    "\n",
    "# Plot correlation heatmap\n",
    "fig, ax = plt.subplots(figsize=(18, 16))\n",
    "\n",
    "im = ax.imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1, aspect='auto')\n",
    "\n",
    "# Set ticks\n",
    "ax.set_xticks(np.arange(len(FEATURE_NAMES)))\n",
    "ax.set_yticks(np.arange(len(FEATURE_NAMES)))\n",
    "ax.set_xticklabels(FEATURE_NAMES, rotation=90, fontsize=9)\n",
    "ax.set_yticklabels(FEATURE_NAMES, fontsize=9)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label('Correlation', rotation=270, labelpad=20)\n",
    "\n",
    "# Add title\n",
    "ax.set_title('Feature Correlation Matrix (All Parameters)', \n",
    "            fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# Add grid lines at group boundaries\n",
    "group_boundaries = [0]\n",
    "for group_indices in FEATURE_GROUPS.values():\n",
    "    group_boundaries.append(group_boundaries[-1] + len(group_indices))\n",
    "\n",
    "for boundary in group_boundaries[1:-1]:\n",
    "    ax.axhline(boundary - 0.5, color='black', linewidth=2)\n",
    "    ax.axvline(boundary - 0.5, color='black', linewidth=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find highly correlated feature pairs (|corr| > 0.9)\n",
    "high_corr_pairs = []\n",
    "for i in range(len(FEATURE_NAMES)):\n",
    "    for j in range(i+1, len(FEATURE_NAMES)):\n",
    "        if abs(corr_matrix[i, j]) > 0.9:\n",
    "            high_corr_pairs.append((FEATURE_NAMES[i], FEATURE_NAMES[j], corr_matrix[i, j]))\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(\"\\nHighly Correlated Features (|r| > 0.9):\")\n",
    "    print(\"=\"*70)\n",
    "    for feat1, feat2, corr in sorted(high_corr_pairs, key=lambda x: abs(x[2]), reverse=True):\n",
    "        print(f\"  {feat1:30s} <-> {feat2:30s}: r={corr:+.3f}\")\n",
    "    print(\"\\nNote: Highly correlated features may be redundant for inference.\")\n",
    "else:\n",
    "    print(\"\\n✓ No highly correlated features (|r| > 0.9)\")\n",
    "    print(\"  This is good - features are relatively independent!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbi_ddm_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
