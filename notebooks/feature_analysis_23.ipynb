{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Feature Visualizations (31 Features)\n",
    "\n",
    "This notebook visualizes how all 31 summary statistics respond to changes in DDM parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Force CPU backend on Apple Silicon to avoid Metal issues\n",
    "os.environ['JAX_PLATFORMS'] = 'cpu'\n",
    "\n",
    "# === Setup ===\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from aind_behavior_vrforaging_analysis.sbi_ddm_analysis.simulator import PatchForagingDDM_JAX, create_prior\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(1)\n",
    "rng_key = random.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_feature_importance_analysis(simulator, prior_fn, feature_names, n_samples=2000):\n",
    "    \"\"\"\n",
    "    Comprehensive analysis to identify most informative features for each parameter\n",
    "    \n",
    "    Returns rankings, plots, and recommendations\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"COMPREHENSIVE FEATURE IMPORTANCE ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 1. GENERATE DATA\n",
    "    # =========================================================================\n",
    "    print(f\"\\nGenerating {n_samples} samples from prior...\")\n",
    "    \n",
    "    rng_key = random.PRNGKey(42)\n",
    "    thetas = []\n",
    "    stats = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        if i % 200 == 0:\n",
    "            print(f\"  {i}/{n_samples}...\")\n",
    "        \n",
    "        rng_key, subkey1, subkey2 = random.split(rng_key, 3)\n",
    "        theta = prior_fn().sample(seed=subkey1)['theta']\n",
    "        _, summary_stats = simulator.simulate_one_window(theta, subkey2)\n",
    "        \n",
    "        thetas.append(theta)\n",
    "        stats.append(summary_stats)\n",
    "    \n",
    "    thetas = jnp.array(thetas)\n",
    "    stats = jnp.array(stats)\n",
    "    \n",
    "    param_names = ['drift_rate', 'reward_bump', 'failure_bump', 'noise_std']\n",
    "    n_features = len(feature_names)\n",
    "    n_params = len(param_names)\n",
    "    \n",
    "    print(f\"\\n✓ Generated data shape: thetas={thetas.shape}, stats={stats.shape}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 2. CORRELATION ANALYSIS (LINEAR RELATIONSHIPS)\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"METHOD 1: CORRELATION ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    correlation_results = {}\n",
    "    \n",
    "    for i, param in enumerate(param_names):\n",
    "        correlations = []\n",
    "        for j in range(n_features):\n",
    "            # Pearson correlation (linear)\n",
    "            r_pearson, _ = pearsonr(np.array(thetas[:, i]), np.array(stats[:, j]))\n",
    "            # Spearman correlation (monotonic)\n",
    "            r_spearman, _ = spearmanr(np.array(thetas[:, i]), np.array(stats[:, j]))\n",
    "            \n",
    "            correlations.append({\n",
    "                'feature': feature_names[j],\n",
    "                'feature_idx': j,\n",
    "                'pearson': r_pearson,\n",
    "                'spearman': r_spearman,\n",
    "                'abs_pearson': abs(r_pearson),\n",
    "                'abs_spearman': abs(r_spearman),\n",
    "            })\n",
    "        \n",
    "        # Sort by absolute correlation\n",
    "        correlations.sort(key=lambda x: x['abs_spearman'], reverse=True)\n",
    "        correlation_results[param] = correlations\n",
    "        \n",
    "        print(f\"\\n{param} - Top 10 correlated features:\")\n",
    "        print(f\"  {'Rank':<6} {'Feature':<30} {'Pearson':>10} {'Spearman':>10}\")\n",
    "        print(\"  \" + \"-\"*60)\n",
    "        for rank, c in enumerate(correlations[:10], 1):\n",
    "            print(f\"  {rank:<6} {c['feature']:<30} {c['pearson']:>10.3f} {c['spearman']:>10.3f}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 3. MUTUAL INFORMATION (NONLINEAR RELATIONSHIPS)\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"METHOD 2: MUTUAL INFORMATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    from sklearn.feature_selection import mutual_info_regression\n",
    "    \n",
    "    mi_results = {}\n",
    "    \n",
    "    for i, param in enumerate(param_names):\n",
    "        # Compute MI for all features\n",
    "        mi_scores = mutual_info_regression(\n",
    "            np.array(stats), \n",
    "            np.array(thetas[:, i]),\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        mi_list = [\n",
    "            {'feature': feature_names[j], 'feature_idx': j, 'mi': mi_scores[j]}\n",
    "            for j in range(n_features)\n",
    "        ]\n",
    "        mi_list.sort(key=lambda x: x['mi'], reverse=True)\n",
    "        mi_results[param] = mi_list\n",
    "        \n",
    "        print(f\"\\n{param} - Top 10 by Mutual Information:\")\n",
    "        print(f\"  {'Rank':<6} {'Feature':<30} {'MI Score':>12}\")\n",
    "        print(\"  \" + \"-\"*50)\n",
    "        for rank, m in enumerate(mi_list[:10], 1):\n",
    "            print(f\"  {rank:<6} {m['feature']:<30} {m['mi']:>12.4f}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 4. RANDOM FOREST FEATURE IMPORTANCE\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"METHOD 3: RANDOM FOREST IMPORTANCE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    rf_results = {}\n",
    "    \n",
    "    for i, param in enumerate(param_names):\n",
    "        print(f\"\\nTraining Random Forest for {param}...\")\n",
    "        \n",
    "        # Train Random Forest\n",
    "        rf = RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        rf.fit(np.array(stats), np.array(thetas[:, i]))\n",
    "        \n",
    "        # Get feature importances\n",
    "        importances = rf.feature_importances_\n",
    "        \n",
    "        # Permutation importance (more robust)\n",
    "        perm_importance = permutation_importance(\n",
    "            rf, np.array(stats), np.array(thetas[:, i]),\n",
    "            n_repeats=10, random_state=42, n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        rf_list = [\n",
    "            {\n",
    "                'feature': feature_names[j],\n",
    "                'feature_idx': j,\n",
    "                'importance': importances[j],\n",
    "                'perm_importance': perm_importance.importances_mean[j],\n",
    "                'perm_std': perm_importance.importances_std[j],\n",
    "            }\n",
    "            for j in range(n_features)\n",
    "        ]\n",
    "        rf_list.sort(key=lambda x: x['perm_importance'], reverse=True)\n",
    "        rf_results[param] = rf_list\n",
    "        \n",
    "        print(f\"\\n{param} - Top 10 by Random Forest:\")\n",
    "        print(f\"  {'Rank':<6} {'Feature':<30} {'Importance':>12} {'Perm.Imp':>12}\")\n",
    "        print(\"  \" + \"-\"*62)\n",
    "        for rank, r in enumerate(rf_list[:10], 1):\n",
    "            print(f\"  {rank:<6} {r['feature']:<30} {r['importance']:>12.4f} \"\n",
    "                  f\"{r['perm_importance']:>12.4f}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 5. LASSO REGRESSION (SPARSE FEATURE SELECTION)\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"METHOD 4: LASSO SPARSE SELECTION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    from sklearn.linear_model import LassoCV\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    stats_scaled = scaler.fit_transform(np.array(stats))\n",
    "    \n",
    "    lasso_results = {}\n",
    "    \n",
    "    for i, param in enumerate(param_names):\n",
    "        print(f\"\\nRunning LASSO for {param}...\")\n",
    "        \n",
    "        # LASSO with cross-validation\n",
    "        lasso = LassoCV(cv=5, random_state=42, max_iter=5000)\n",
    "        lasso.fit(stats_scaled, np.array(thetas[:, i]))\n",
    "        \n",
    "        # Get non-zero coefficients\n",
    "        lasso_list = [\n",
    "            {\n",
    "                'feature': feature_names[j],\n",
    "                'feature_idx': j,\n",
    "                'coefficient': lasso.coef_[j],\n",
    "                'abs_coefficient': abs(lasso.coef_[j]),\n",
    "            }\n",
    "            for j in range(n_features)\n",
    "        ]\n",
    "        lasso_list.sort(key=lambda x: x['abs_coefficient'], reverse=True)\n",
    "        lasso_results[param] = lasso_list\n",
    "        \n",
    "        # Count selected features\n",
    "        n_selected = sum(1 for l in lasso_list if abs(l['coefficient']) > 0.001)\n",
    "        \n",
    "        print(f\"\\n{param} - LASSO selected {n_selected}/{n_features} features\")\n",
    "        print(f\"  Alpha: {lasso.alpha_:.6f}, R²: {lasso.score(stats_scaled, np.array(thetas[:, i])):.3f}\")\n",
    "        print(f\"\\n  {'Rank':<6} {'Feature':<30} {'Coefficient':>15}\")\n",
    "        print(\"  \" + \"-\"*53)\n",
    "        for rank, l in enumerate([x for x in lasso_list if abs(x['coefficient']) > 0.001][:10], 1):\n",
    "            print(f\"  {rank:<6} {l['feature']:<30} {l['coefficient']:>15.6f}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 6. ENSEMBLE RANKING (COMBINE ALL METHODS)\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"METHOD 5: ENSEMBLE RANKING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    ensemble_results = {}\n",
    "    \n",
    "    for param in param_names:\n",
    "        # Collect rankings from each method\n",
    "        feature_scores = {fname: {'ranks': [], 'scores': []} for fname in feature_names}\n",
    "        \n",
    "        # Add correlation ranks\n",
    "        for rank, item in enumerate(correlation_results[param], 1):\n",
    "            feature_scores[item['feature']]['ranks'].append(rank)\n",
    "            feature_scores[item['feature']]['scores'].append(item['abs_spearman'])\n",
    "        \n",
    "        # Add MI ranks\n",
    "        for rank, item in enumerate(mi_results[param], 1):\n",
    "            feature_scores[item['feature']]['ranks'].append(rank)\n",
    "            feature_scores[item['feature']]['scores'].append(item['mi'])\n",
    "        \n",
    "        # Add RF ranks\n",
    "        for rank, item in enumerate(rf_results[param], 1):\n",
    "            feature_scores[item['feature']]['ranks'].append(rank)\n",
    "            feature_scores[item['feature']]['scores'].append(item['perm_importance'])\n",
    "        \n",
    "        # Add LASSO ranks\n",
    "        for rank, item in enumerate(lasso_results[param], 1):\n",
    "            feature_scores[item['feature']]['ranks'].append(rank)\n",
    "            feature_scores[item['feature']]['scores'].append(item['abs_coefficient'])\n",
    "        \n",
    "        # Compute ensemble score (average rank, lower is better)\n",
    "        ensemble_list = []\n",
    "        for fname, data in feature_scores.items():\n",
    "            avg_rank = np.mean(data['ranks'])\n",
    "            min_rank = min(data['ranks'])\n",
    "            max_rank = max(data['ranks'])\n",
    "            \n",
    "            ensemble_list.append({\n",
    "                'feature': fname,\n",
    "                'avg_rank': avg_rank,\n",
    "                'min_rank': min_rank,\n",
    "                'max_rank': max_rank,\n",
    "                'rank_std': np.std(data['ranks']),\n",
    "            })\n",
    "        \n",
    "        ensemble_list.sort(key=lambda x: x['avg_rank'])\n",
    "        ensemble_results[param] = ensemble_list\n",
    "        \n",
    "        print(f\"\\n{param} - ENSEMBLE RANKING (lower rank = more important):\")\n",
    "        print(f\"  {'Rank':<6} {'Feature':<30} {'Avg Rank':>10} {'Best':>6} {'Worst':>6} {'Std':>6}\")\n",
    "        print(\"  \" + \"-\"*70)\n",
    "        for rank, e in enumerate(ensemble_list[:15], 1):\n",
    "            print(f\"  {rank:<6} {e['feature']:<30} {e['avg_rank']:>10.1f} \"\n",
    "                  f\"{e['min_rank']:>6.0f} {e['max_rank']:>6.0f} {e['rank_std']:>6.1f}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 7. VISUALIZATIONS\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"GENERATING VISUALIZATIONS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Plot 1: Heatmap of feature importance across methods\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "    \n",
    "    for idx, (ax, param) in enumerate(zip(axes.flat, param_names)):\n",
    "        # Get top 15 features from ensemble\n",
    "        top_features = [e['feature'] for e in ensemble_results[param][:15]]\n",
    "        \n",
    "        # Create importance matrix\n",
    "        methods = ['Correlation', 'Mutual Info', 'Random Forest', 'LASSO']\n",
    "        importance_matrix = np.zeros((len(top_features), len(methods)))\n",
    "        \n",
    "        for i, fname in enumerate(top_features):\n",
    "            # Correlation (normalized)\n",
    "            corr_item = next(c for c in correlation_results[param] if c['feature'] == fname)\n",
    "            importance_matrix[i, 0] = corr_item['abs_spearman']\n",
    "            \n",
    "            # MI (normalized)\n",
    "            mi_item = next(m for m in mi_results[param] if m['feature'] == fname)\n",
    "            importance_matrix[i, 1] = mi_item['mi'] / max(m['mi'] for m in mi_results[param])\n",
    "            \n",
    "            # RF (normalized)\n",
    "            rf_item = next(r for r in rf_results[param] if r['feature'] == fname)\n",
    "            importance_matrix[i, 2] = rf_item['perm_importance'] / max(r['perm_importance'] for r in rf_results[param])\n",
    "            \n",
    "            # LASSO (normalized)\n",
    "            lasso_item = next(l for l in lasso_results[param] if l['feature'] == fname)\n",
    "            importance_matrix[i, 3] = lasso_item['abs_coefficient'] / max(l['abs_coefficient'] for l in lasso_results[param])\n",
    "        \n",
    "        sns.heatmap(importance_matrix, ax=ax, cmap='YlOrRd', \n",
    "                   xticklabels=methods, yticklabels=top_features,\n",
    "                   cbar_kws={'label': 'Normalized Importance'},\n",
    "                   vmin=0, vmax=1, annot=True, fmt='.2f')\n",
    "        ax.set_title(f'{param} - Top 15 Features', fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('Method', fontsize=12)\n",
    "        ax.set_ylabel('Feature', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_importance_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"✓ Saved: feature_importance_heatmap.png\")\n",
    "    \n",
    "    # Plot 2: Scatter plots of top features vs parameters\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(20, 16))\n",
    "    \n",
    "    for i, param in enumerate(param_names):\n",
    "        top_4_features = [e['feature'] for e in ensemble_results[param][:4]]\n",
    "        \n",
    "        for j, fname in enumerate(top_4_features):\n",
    "            ax = axes[i, j]\n",
    "            feat_idx = feature_names.index(fname)\n",
    "            \n",
    "            ax.scatter(stats[:, feat_idx], thetas[:, i], alpha=0.3, s=1)\n",
    "            ax.set_xlabel(fname, fontsize=10)\n",
    "            ax.set_ylabel(param if j == 0 else '', fontsize=10)\n",
    "            \n",
    "            # Add correlation\n",
    "            r = correlation_results[param][next(k for k, c in enumerate(correlation_results[param]) if c['feature'] == fname)]['spearman']\n",
    "            ax.set_title(f'ρ={r:.3f}', fontsize=9)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Top 4 Features for Each Parameter', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('top_features_scatter.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"✓ Saved: top_features_scatter.png\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 8. RECOMMENDATIONS\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FEATURE SELECTION RECOMMENDATIONS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Find features that appear in top 10 for multiple parameters\n",
    "    feature_frequency = {}\n",
    "    for fname in feature_names:\n",
    "        count = sum(\n",
    "            1 for param in param_names \n",
    "            if fname in [e['feature'] for e in ensemble_results[param][:10]]\n",
    "        )\n",
    "        if count > 0:\n",
    "            feature_frequency[fname] = count\n",
    "    \n",
    "    print(\"\\nFeatures appearing in top 10 for multiple parameters:\")\n",
    "    for fname, count in sorted(feature_frequency.items(), key=lambda x: x[1], reverse=True):\n",
    "        params = [p for p in param_names if fname in [e['feature'] for e in ensemble_results[p][:10]]]\n",
    "        print(f\"  {fname:<30} → {count} parameters: {', '.join(params)}\")\n",
    "    \n",
    "    # Recommend minimal feature set\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"RECOMMENDED MINIMAL FEATURE SETS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for n_features_target in [10, 15, 20]:\n",
    "        print(f\"\\nTop {n_features_target} features (union across all parameters):\")\n",
    "        \n",
    "        # Collect top N from each parameter\n",
    "        selected = set()\n",
    "        for param in param_names:\n",
    "            selected.update([e['feature'] for e in ensemble_results[param][:n_features_target//2]])\n",
    "        \n",
    "        # Sort by frequency\n",
    "        selected_list = sorted(\n",
    "            selected,\n",
    "            key=lambda x: sum(1 for p in param_names if x in [e['feature'] for e in ensemble_results[p][:10]]),\n",
    "            reverse=True\n",
    "        )[:n_features_target]\n",
    "        \n",
    "        print(f\"  Total: {len(selected_list)} features\")\n",
    "        for fname in selected_list:\n",
    "            # Find which parameters this feature is top-10 for\n",
    "            good_for = [p for p in param_names if fname in [e['feature'] for e in ensemble_results[p][:10]]]\n",
    "            print(f\"    {fname:<30} → {', '.join(good_for) if good_for else 'general'}\")\n",
    "    \n",
    "    return {\n",
    "        'correlation': correlation_results,\n",
    "        'mutual_info': mi_results,\n",
    "        'random_forest': rf_results,\n",
    "        'lasso': lasso_results,\n",
    "        'ensemble': ensemble_results,\n",
    "        'data': (thetas, stats)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 35-feature model\n",
    "from aind_behavior_vrforaging_analysis.sbi_ddm_analysis.enhanced_stats_35 import FEATURE_NAMES  # 35 features\n",
    "\n",
    "\n",
    "# Initialize simulator\n",
    "simulator_35 = PatchForagingDDM_JAX(max_sites_per_window=100)\n",
    "prior_fn = create_prior()\n",
    "\n",
    "results_35 = comprehensive_feature_importance_analysis(\n",
    "    simulator_35,  # Your 35-feature simulator\n",
    "    prior_fn,\n",
    "    FEATURE_NAMES,\n",
    "    n_samples=2000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 37-feature model\n",
    "from aind_behavior_vrforaging_analysis.sbi_ddm_analysis.enhanced_stats_37 import FEATURE_NAMES  # 37 features\n",
    "\n",
    "\n",
    "# Initialize simulator\n",
    "simulator_37 = PatchForagingDDM_JAX(max_sites_per_window=100)\n",
    "prior_fn = create_prior()\n",
    "\n",
    "results_37 = comprehensive_feature_importance_analysis(\n",
    "    simulator_37,  # Your 37-feature simulator\n",
    "    prior_fn,\n",
    "    FEATURE_NAMES,\n",
    "    n_samples=2000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 23-feature model\n",
    "from aind_behavior_vrforaging_analysis.sbi_ddm_analysis.enhanced_stats_23 import FEATURE_NAMES  # 23 features\n",
    "\n",
    "# Initialize simulator\n",
    "simulator_23 = PatchForagingDDM_JAX(max_sites_per_window=100)\n",
    "prior_fn = create_prior()\n",
    "\n",
    "results_23 = comprehensive_feature_importance_analysis(\n",
    "    simulator_23,  # Your 23-feature simulator\n",
    "    prior_fn,\n",
    "    FEATURE_NAMES,\n",
    "    n_samples=2000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aind_behavior_vrforaging_analysis.sbi_ddm_analysis.enhanced_stats import compute_summary_stats\n",
    "\n",
    "# Test sensitivity of new features\n",
    "def test_new_features():\n",
    "    rng_key = random.PRNGKey(42)\n",
    "    \n",
    "    test_cases = {\n",
    "        'High drift vs low drift': [\n",
    "            jnp.array([1.5, 0.5, 0.5, 0.2]),\n",
    "            jnp.array([0.3, 0.5, 0.5, 0.2]),\n",
    "        ],\n",
    "        'High reward_bump vs low': [\n",
    "            jnp.array([0.75, 1.5, 0.5, 0.2]),\n",
    "            jnp.array([0.75, 0.2, 0.5, 0.2]),\n",
    "        ],\n",
    "        'High failure_bump vs low': [\n",
    "            jnp.array([0.75, 0.5, 1.5, 0.2]),\n",
    "            jnp.array([0.75, 0.5, 0.2, 0.2]),\n",
    "        ],\n",
    "        'High noise vs low': [\n",
    "            jnp.array([0.75, 0.5, 0.5, 0.4]),\n",
    "            jnp.array([0.75, 0.5, 0.5, 0.05]),\n",
    "        ],\n",
    "    }\n",
    "    \n",
    "    for test_name, (theta1, theta2) in test_cases.items():\n",
    "        print(f\"\\n{test_name}:\")\n",
    "        \n",
    "        # Simulate from each\n",
    "        stats1_list = []\n",
    "        stats2_list = []\n",
    "        \n",
    "        for _ in range(50):\n",
    "            rng_key, sub1, sub2 = random.split(rng_key, 3)\n",
    "            \n",
    "            window_data1, _, _ = simulate_and_extract(theta1, sub1)\n",
    "            stats1 = compute_summary_stats(window_data1)\n",
    "            stats1_list.append(stats1)\n",
    "            \n",
    "            window_data2, _, _ = simulate_and_extract(theta2, sub2)\n",
    "            stats2 = compute_summary_stats(window_data2)\n",
    "            stats2_list.append(stats2)\n",
    "        \n",
    "        stats1_mean = jnp.mean(jnp.array(stats1_list), axis=0)\n",
    "        stats2_mean = jnp.mean(jnp.array(stats2_list), axis=0)\n",
    "        \n",
    "        # Find features with biggest difference\n",
    "        diffs = jnp.abs(stats1_mean - stats2_mean)\n",
    "        top_5 = jnp.argsort(diffs)[::-1][:5]\n",
    "        \n",
    "        print(\"  Top 5 discriminating features:\")\n",
    "        for rank, idx in enumerate(top_5, 1):\n",
    "            print(f\"    {rank}. {OPTIMIZED_FEATURE_NAMES[idx]:<30} \"\n",
    "                  f\"({stats1_mean[idx]:.3f} vs {stats2_mean[idx]:.3f})\")\n",
    "\n",
    "test_new_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "def analyze_summary_stat_correlations(n_samples=1000):\n",
    "    \"\"\"\n",
    "    Generate samples from prior, compute correlations, visualize redundancy\n",
    "    \"\"\"\n",
    "    print(\"Generating samples from prior...\")\n",
    "    rng_key = random.PRNGKey(42)\n",
    "    prior_fn = create_prior()\n",
    "    \n",
    "    # Sample from prior\n",
    "    prior_dist = prior_fn()\n",
    "    theta_samples = prior_dist.sample(n_samples, seed=rng_key)['theta']\n",
    "    \n",
    "    # Simulate and extract stats\n",
    "    all_stats = []\n",
    "    for i in range(n_samples):\n",
    "        rng_key, subkey = random.split(rng_key)\n",
    "        _, stats, _ = simulate_and_extract(theta_samples[i], subkey)\n",
    "        all_stats.append(stats)\n",
    "    \n",
    "    all_stats = np.array(all_stats)  # (1000, 37)\n",
    "    \n",
    "    # Compute correlation matrix\n",
    "    corr_matrix = np.corrcoef(all_stats.T)\n",
    "    \n",
    "    # === PLOT 1: Full correlation heatmap ===\n",
    "    fig, ax = plt.subplots(figsize=(16, 14))\n",
    "    im = ax.imshow(corr_matrix, cmap='RdBu_r', vmin=-1, vmax=1, aspect='auto')\n",
    "    \n",
    "    # Add feature names\n",
    "    ax.set_xticks(range(len(FEATURE_NAMES)))\n",
    "    ax.set_yticks(range(len(FEATURE_NAMES)))\n",
    "    ax.set_xticklabels(FEATURE_NAMES, rotation=90, ha='right', fontsize=8)\n",
    "    ax.set_yticklabels(FEATURE_NAMES, fontsize=8)\n",
    "    \n",
    "    # Add colorbar\n",
    "    plt.colorbar(im, ax=ax, label='Correlation')\n",
    "    \n",
    "    # Add grid lines between groups\n",
    "    group_boundaries = [0, 7, 11, 16, 20, 23, 26, 37]\n",
    "    for boundary in group_boundaries:\n",
    "        ax.axhline(boundary - 0.5, color='black', linewidth=1.5)\n",
    "        ax.axvline(boundary - 0.5, color='black', linewidth=1.5)\n",
    "    \n",
    "    plt.title('Summary Statistics Correlation Matrix', fontsize=14, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('correlation_matrix_full.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # === PLOT 2: Hierarchical clustering to find redundant groups ===\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Convert correlation to distance\n",
    "    distance_matrix = 1 - np.abs(corr_matrix)\n",
    "    linkage_matrix = linkage(distance_matrix, method='ward')\n",
    "    \n",
    "    dendrogram(linkage_matrix, labels=FEATURE_NAMES, ax=ax, \n",
    "               orientation='right', leaf_font_size=8)\n",
    "    ax.set_xlabel('Distance (1 - |correlation|)')\n",
    "    ax.set_title('Hierarchical Clustering of Summary Statistics')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('stat_clustering.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # === IDENTIFY HIGHLY CORRELATED PAIRS ===\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"HIGHLY CORRELATED PAIRS (|r| > 0.95)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(FEATURE_NAMES)):\n",
    "        for j in range(i+1, len(FEATURE_NAMES)):\n",
    "            if abs(corr_matrix[i, j]) > 0.95:\n",
    "                high_corr_pairs.append((i, j, corr_matrix[i, j]))\n",
    "    \n",
    "    high_corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "    \n",
    "    for i, j, r in high_corr_pairs:\n",
    "        print(f\"{FEATURE_NAMES[i]:<30} <-> {FEATURE_NAMES[j]:<30}  r = {r:>6.3f}\")\n",
    "    \n",
    "    print(f\"\\nTotal highly correlated pairs: {len(high_corr_pairs)}\")\n",
    "    \n",
    "    # === RECOMMEND REMOVALS ===\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"RECOMMENDED REMOVALS (keep one from each pair)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    to_remove = set()\n",
    "    for i, j, r in high_corr_pairs:\n",
    "        if i not in to_remove and j not in to_remove:\n",
    "            # Keep the one that appears in fewer pairs\n",
    "            i_count = sum(1 for x, y, _ in high_corr_pairs if i in (x, y))\n",
    "            j_count = sum(1 for x, y, _ in high_corr_pairs if j in (x, y))\n",
    "            \n",
    "            if j_count > i_count:\n",
    "                to_remove.add(j)\n",
    "                print(f\"Remove: {FEATURE_NAMES[j]:<30} (redundant with {FEATURE_NAMES[i]})\")\n",
    "            else:\n",
    "                to_remove.add(i)\n",
    "                print(f\"Remove: {FEATURE_NAMES[i]:<30} (redundant with {FEATURE_NAMES[j]})\")\n",
    "    \n",
    "    print(f\"\\nRecommended to remove: {len(to_remove)} stats\")\n",
    "    print(f\"Would reduce from {len(FEATURE_NAMES)} -> {len(FEATURE_NAMES) - len(to_remove)} stats\")\n",
    "    \n",
    "    return all_stats, corr_matrix, list(to_remove)\n",
    "\n",
    "# Run the analysis\n",
    "all_stats, corr_matrix, to_remove = analyze_summary_stat_correlations(n_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_sensitivity_analysis():\n",
    "    \"\"\"\n",
    "    For each parameter, vary it while holding others constant.\n",
    "    See which summary stats are most sensitive.\n",
    "    \"\"\"\n",
    "    rng_key = random.PRNGKey(42)\n",
    "    \n",
    "    # Base parameter values (middle of prior)\n",
    "    base_theta = jnp.array([1.0, 1.0, 1.0, 0.275])\n",
    "    \n",
    "    param_names = [\"drift_rate\", \"reward_bump\", \"failure_bump\", \"noise_std\"]\n",
    "    param_ranges = {\n",
    "        'drift_rate': jnp.linspace(0.0, 2.0, 15),\n",
    "        'reward_bump': jnp.linspace(0.0, 2.0, 15),\n",
    "        'failure_bump': jnp.linspace(0.0, 2.0, 15),\n",
    "        'noise_std': jnp.linspace(0.05, 0.5, 15),\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for param_idx, (param_name, param_values) in enumerate(param_ranges.items()):\n",
    "        print(f\"\\nAnalyzing sensitivity to {param_name}...\")\n",
    "        \n",
    "        stats_for_param = []\n",
    "        \n",
    "        for param_val in param_values:\n",
    "            # Create theta with one parameter varied\n",
    "            theta = base_theta.at[param_idx].set(param_val)\n",
    "            \n",
    "            # Average over 50 simulations to reduce noise\n",
    "            stats_samples = []\n",
    "            for _ in range(50):\n",
    "                rng_key, subkey = random.split(rng_key)\n",
    "                _, stats, _ = simulate_and_extract(theta, subkey)\n",
    "                stats_samples.append(stats)\n",
    "            \n",
    "            mean_stats = np.mean(stats_samples, axis=0)\n",
    "            stats_for_param.append(mean_stats)\n",
    "        \n",
    "        results[param_name] = {\n",
    "            'values': param_values,\n",
    "            'stats': np.array(stats_for_param)  # (15, 37)\n",
    "        }\n",
    "    \n",
    "    # === COMPUTE SENSITIVITY SCORES ===\n",
    "    from scipy.stats import spearmanr\n",
    "    \n",
    "    sensitivity_matrix = np.zeros((4, len(FEATURE_NAMES)))\n",
    "    \n",
    "    for param_idx, param_name in enumerate(param_names):\n",
    "        param_values = results[param_name]['values']\n",
    "        param_stats = results[param_name]['stats']\n",
    "        \n",
    "        for stat_idx in range(len(FEATURE_NAMES)):\n",
    "            # Spearman correlation between parameter value and stat value\n",
    "            corr, _ = spearmanr(param_values, param_stats[:, stat_idx])\n",
    "            sensitivity_matrix[param_idx, stat_idx] = abs(corr)\n",
    "    \n",
    "    # === PLOT SENSITIVITY HEATMAP ===\n",
    "    fig, ax = plt.subplots(figsize=(16, 6))\n",
    "    im = ax.imshow(sensitivity_matrix, cmap='YlOrRd', aspect='auto', vmin=0, vmax=1)\n",
    "    \n",
    "    ax.set_yticks(range(4))\n",
    "    ax.set_yticklabels(param_names, fontsize=10)\n",
    "    ax.set_xticks(range(len(FEATURE_NAMES)))\n",
    "    ax.set_xticklabels(FEATURE_NAMES, rotation=90, ha='right', fontsize=8)\n",
    "    \n",
    "    plt.colorbar(im, ax=ax, label='|Spearman correlation|')\n",
    "    plt.title('Parameter Sensitivity: Which stats respond to which parameters?', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('parameter_sensitivity.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # === IDENTIFY MOST SENSITIVE STATS PER PARAMETER ===\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TOP 5 MOST SENSITIVE STATS FOR EACH PARAMETER\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for param_idx, param_name in enumerate(param_names):\n",
    "        print(f\"\\n{param_name.upper()}:\")\n",
    "        \n",
    "        # Get top 5 stats\n",
    "        top_indices = np.argsort(sensitivity_matrix[param_idx])[::-1][:5]\n",
    "        \n",
    "        for rank, stat_idx in enumerate(top_indices, 1):\n",
    "            corr = sensitivity_matrix[param_idx, stat_idx]\n",
    "            print(f\"  {rank}. {FEATURE_NAMES[stat_idx]:<30} (|r| = {corr:.3f})\")\n",
    "    \n",
    "    # === IDENTIFY WEAK STATS (not sensitive to anything) ===\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"WEAK STATS (max sensitivity < 0.3 to any parameter)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    max_sensitivity = sensitivity_matrix.max(axis=0)\n",
    "    weak_stats = np.where(max_sensitivity < 0.3)[0]\n",
    "    \n",
    "    for stat_idx in weak_stats:\n",
    "        print(f\"{FEATURE_NAMES[stat_idx]:<30} (max |r| = {max_sensitivity[stat_idx]:.3f})\")\n",
    "    \n",
    "    return results, sensitivity_matrix\n",
    "\n",
    "# Run sensitivity analysis\n",
    "sensitivity_results, sensitivity_matrix = parameter_sensitivity_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Sweep Setup ===\n",
    "theta_labels = [\"drift_rate\", \"reward_bump\", \"failure_bump\", \"noise_std\"]\n",
    "theta_base = jnp.array([0.4, 0.3, 0.1, 0.1])  # Base parameters\n",
    "\n",
    "n_repeats = 20  # Runs per parameter combination\n",
    "gradient_values = np.linspace(0.01, 1.0, 5)  # 5 levels for gradient\n",
    "x_values = np.linspace(0.01, 1.0, 10)  # 10 points for x-axis\n",
    "\n",
    "# Storage for results\n",
    "results_mean = {label: {} for label in theta_labels}\n",
    "results_sem = {label: {} for label in theta_labels}\n",
    "\n",
    "print(f\"Will run {len(theta_labels)} parameter sweeps\")\n",
    "print(f\"Each sweep: {len(gradient_values)} gradients × {len(x_values)} x-values × {n_repeats} repeats\")\n",
    "print(f\"Total simulations: {len(theta_labels) * len(gradient_values) * len(x_values) * n_repeats}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Run Parameter Sweeps ===\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for param_idx, param_x in enumerate(theta_labels):\n",
    "    print(f\"\\nSweeping {param_x} ({param_idx+1}/{len(theta_labels)})...\")\n",
    "    \n",
    "    # Get other parameters (for gradient and fixed)\n",
    "    other_params = [p for j, p in enumerate(theta_labels) if j != param_idx]\n",
    "    gradient_param_idx = theta_labels.index(other_params[0])  # First other param varies\n",
    "    \n",
    "    for grad_val in gradient_values:\n",
    "        mean_list, sem_list = [], []\n",
    "        \n",
    "        for x_val in x_values:\n",
    "            # Build theta for this combination\n",
    "            theta = theta_base.copy()\n",
    "            theta = theta.at[param_idx].set(x_val)  # X-axis parameter\n",
    "            theta = theta.at[gradient_param_idx].set(grad_val)  # Gradient parameter\n",
    "            # Other parameters stay at base values\n",
    "            \n",
    "            # Run multiple simulations\n",
    "            runs = []\n",
    "            for _ in range(n_repeats):\n",
    "                _, summary, rng_key = simulate_and_extract(theta, rng_key)\n",
    "                runs.append(np.array(summary))\n",
    "            \n",
    "            runs = np.vstack(runs)\n",
    "            mean_list.append(runs.mean(axis=0))\n",
    "            sem_list.append(runs.std(axis=0, ddof=1) / np.sqrt(n_repeats))\n",
    "        \n",
    "        # Store results\n",
    "        key = f\"{grad_val:.4f}\"\n",
    "        results_mean[param_x][key] = np.vstack(mean_list)\n",
    "        results_sem[param_x][key] = np.vstack(sem_list)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"  Completed in {elapsed:.1f}s\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n✓ All sweeps completed in {total_time/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 1: Key Features by Parameter\n",
    "\n",
    "Focus on the most important features for each parameter:\n",
    "- **drift_rate**: Basic time statistics, temporal trends\n",
    "- **reward_bump**: Reward history effects (mean_time_after_reward)\n",
    "- **failure_bump**: Reward history effects (mean_time_after_failure)\n",
    "- **noise_std**: Distribution shape, sequential dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Visualization 1: Most Important Features ===\n",
    "\n",
    "# Define key features to visualize for each parameter\n",
    "KEY_FEATURES = {\n",
    "    \"drift_rate\": [1, 2, 11, 12, 13],  # mean_time, std_time, early_mean, late_mean, trend\n",
    "    \"reward_bump\": [7, 8, 9, 14, 23],  # After reward/failure, late-early, reward_rate\n",
    "    \"failure_bump\": [7, 8, 9, 10, 14], # After reward/failure stats\n",
    "    \"noise_std\": [2, 16, 18, 19, 20],  # std_time, p25, p75, iqr, autocorr\n",
    "}\n",
    "\n",
    "for param_x in theta_labels:\n",
    "    other_params = [p for j, p in enumerate(theta_labels) if p != param_x]\n",
    "    gradient_param = other_params[0]\n",
    "    \n",
    "    # Get key features for this parameter\n",
    "    feature_indices = KEY_FEATURES[param_x]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(feature_indices), figsize=(4*len(feature_indices), 4))\n",
    "    if len(feature_indices) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    fig.suptitle(f\"Effect of {param_x} (gradient: {gradient_param})\", fontsize=14, fontweight='bold')\n",
    "    \n",
    "    cmap = plt.cm.viridis(np.linspace(0, 1, len(gradient_values)))\n",
    "    \n",
    "    for ax_idx, feature_idx in enumerate(feature_indices):\n",
    "        ax = axes[ax_idx]\n",
    "        feature_name = FEATURE_NAMES[feature_idx]\n",
    "        \n",
    "        for color_idx, grad_val in enumerate(gradient_values):\n",
    "            key = f\"{grad_val:.4f}\"\n",
    "            mean_vals = results_mean[param_x][key][:, feature_idx]\n",
    "            sem_vals = results_sem[param_x][key][:, feature_idx]\n",
    "            \n",
    "            ax.plot(x_values, mean_vals, color=cmap[color_idx], \n",
    "                   marker='o', markersize=4, linewidth=2,\n",
    "                   label=f\"{grad_val:.2f}\")\n",
    "            ax.fill_between(x_values,\n",
    "                          mean_vals - 1.96 * sem_vals,\n",
    "                          mean_vals + 1.96 * sem_vals,\n",
    "                          color=cmap[color_idx], alpha=0.2)\n",
    "        \n",
    "        ax.set_title(feature_name, fontsize=11, fontweight='bold')\n",
    "        ax.set_xlabel(param_x, fontsize=10)\n",
    "        ax.set_ylabel('Value', fontsize=10)\n",
    "        ax.grid(True, linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # Add legend\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='center left', bbox_to_anchor=(1, 0.5),\n",
    "              title=gradient_param, fontsize=9)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 0.95, 0.96])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 2: Feature Groups\n",
    "\n",
    "Visualize all features organized by functional groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Visualization 2: All Features by Group ===\n",
    "\n",
    "for param_x in theta_labels:\n",
    "    other_params = [p for j, p in enumerate(theta_labels) if p != param_x]\n",
    "    gradient_param = other_params[0]\n",
    "    \n",
    "    # Create one large figure with subplots for each feature group\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    fig.suptitle(f\"All Features: {param_x} (gradient: {gradient_param})\", \n",
    "                fontsize=16, fontweight='bold', y=0.995)\n",
    "    \n",
    "    cmap = plt.cm.viridis(np.linspace(0, 1, len(gradient_values)))\n",
    "    \n",
    "    # Create subplots for each group\n",
    "    group_row = 0\n",
    "    for group_name, feature_indices in FEATURE_GROUPS.items():\n",
    "        n_features = len(feature_indices)\n",
    "        \n",
    "        # Add group title\n",
    "        ax_title = plt.subplot(8, 7, group_row * 7 + 1)\n",
    "        ax_title.text(0.5, 0.5, group_name, fontsize=12, fontweight='bold',\n",
    "                     ha='center', va='center')\n",
    "        ax_title.axis('off')\n",
    "        \n",
    "        # Plot features in this group\n",
    "        for i, feature_idx in enumerate(feature_indices):\n",
    "            ax = plt.subplot(8, 7, group_row * 7 + i + 2)\n",
    "            feature_name = FEATURE_NAMES[feature_idx]\n",
    "            \n",
    "            for color_idx, grad_val in enumerate(gradient_values):\n",
    "                key = f\"{grad_val:.4f}\"\n",
    "                mean_vals = results_mean[param_x][key][:, feature_idx]\n",
    "                sem_vals = results_sem[param_x][key][:, feature_idx]\n",
    "                \n",
    "                ax.plot(x_values, mean_vals, color=cmap[color_idx], \n",
    "                       marker='o', markersize=3, linewidth=1.5, alpha=0.8)\n",
    "                ax.fill_between(x_values,\n",
    "                              mean_vals - 1.96 * sem_vals,\n",
    "                              mean_vals + 1.96 * sem_vals,\n",
    "                              color=cmap[color_idx], alpha=0.15)\n",
    "            \n",
    "            ax.set_title(feature_name, fontsize=8)\n",
    "            ax.tick_params(labelsize=7)\n",
    "            ax.grid(True, linestyle='--', alpha=0.3)\n",
    "        \n",
    "        group_row += 1\n",
    "    \n",
    "    # Add legend\n",
    "    legend_ax = plt.subplot(7, 7, 49)\n",
    "    for color_idx, grad_val in enumerate(gradient_values):\n",
    "        legend_ax.plot([], [], color=cmap[color_idx], linewidth=3, \n",
    "                      label=f\"{grad_val:.2f}\")\n",
    "    legend_ax.legend(title=gradient_param, loc='center', fontsize=8)\n",
    "    legend_ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.99])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 3: Reward History Effects\n",
    "\n",
    "Deep dive into the critical reward history features that distinguish reward_bump from failure_bump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Visualization 3: Reward History Deep Dive ===\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Reward History Effects: Critical Features for Bump Parameters', \n",
    "            fontsize=14, fontweight='bold')\n",
    "\n",
    "# Feature indices for reward history\n",
    "reward_features = [7, 8, 9, 10]  # After reward mean/std, after failure mean/std\n",
    "params_to_show = ['reward_bump', 'failure_bump']\n",
    "\n",
    "for param_idx, param_x in enumerate(params_to_show):\n",
    "    other_params = [p for j, p in enumerate(theta_labels) if p != param_x]\n",
    "    gradient_param = other_params[0]\n",
    "    \n",
    "    cmap = plt.cm.viridis(np.linspace(0, 1, len(gradient_values)))\n",
    "    \n",
    "    for feat_idx, feature_idx in enumerate(reward_features):\n",
    "        ax = axes[param_idx, feat_idx]\n",
    "        feature_name = FEATURE_NAMES[feature_idx]\n",
    "        \n",
    "        for color_idx, grad_val in enumerate(gradient_values):\n",
    "            key = f\"{grad_val:.4f}\"\n",
    "            mean_vals = results_mean[param_x][key][:, feature_idx]\n",
    "            sem_vals = results_sem[param_x][key][:, feature_idx]\n",
    "            \n",
    "            ax.plot(x_values, mean_vals, color=cmap[color_idx], \n",
    "                   marker='o', markersize=5, linewidth=2.5,\n",
    "                   label=f\"{gradient_param}={grad_val:.2f}\")\n",
    "            ax.fill_between(x_values,\n",
    "                          mean_vals - 1.96 * sem_vals,\n",
    "                          mean_vals + 1.96 * sem_vals,\n",
    "                          color=cmap[color_idx], alpha=0.2)\n",
    "        \n",
    "        ax.set_title(f\"{param_x} → {feature_name}\", fontsize=11, fontweight='bold')\n",
    "        ax.set_xlabel(param_x, fontsize=10)\n",
    "        ax.set_ylabel('Time (s)', fontsize=10)\n",
    "        ax.grid(True, linestyle='--', alpha=0.3)\n",
    "        \n",
    "        if feat_idx == 0:\n",
    "            ax.legend(fontsize=8, loc='best')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insight:\")\n",
    "print(\"  • reward_bump should primarily affect 'mean_time_after_reward' (feature 7)\")\n",
    "print(\"  • failure_bump should primarily affect 'mean_time_after_failure' (feature 8)\")\n",
    "print(\"  • These features enable parameter identifiability!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 4: Feature Sensitivity Heatmap\n",
    "\n",
    "Show which features are most sensitive to each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Visualization 4: Sensitivity Heatmap ===\n",
    "\n",
    "# Compute sensitivity as the range (max - min) of each feature\n",
    "# across parameter values (using middle gradient value)\n",
    "sensitivity_matrix = np.zeros((len(theta_labels), len(FEATURE_NAMES)))\n",
    "\n",
    "middle_gradient_idx = len(gradient_values) // 2\n",
    "middle_gradient_val = gradient_values[middle_gradient_idx]\n",
    "key = f\"{middle_gradient_val:.4f}\"\n",
    "\n",
    "for param_idx, param_x in enumerate(theta_labels):\n",
    "    means = results_mean[param_x][key]\n",
    "    for feat_idx in range(len(FEATURE_NAMES)):\n",
    "        # Compute normalized range\n",
    "        feat_values = means[:, feat_idx]\n",
    "        feat_range = np.max(feat_values) - np.min(feat_values)\n",
    "        feat_mean = np.mean(feat_values)\n",
    "        # Normalize by mean to get relative sensitivity\n",
    "        sensitivity_matrix[param_idx, feat_idx] = feat_range / (np.abs(feat_mean) + 1e-6)\n",
    "\n",
    "# Plot heatmap\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "im = ax.imshow(sensitivity_matrix, aspect='auto', cmap='YlOrRd', interpolation='nearest')\n",
    "\n",
    "# Set ticks and labels\n",
    "ax.set_xticks(np.arange(len(FEATURE_NAMES)))\n",
    "ax.set_yticks(np.arange(len(theta_labels)))\n",
    "ax.set_xticklabels(FEATURE_NAMES, rotation=90, fontsize=8)\n",
    "ax.set_yticklabels(theta_labels, fontsize=10)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label('Relative Sensitivity (Range/Mean)', rotation=270, labelpad=20)\n",
    "\n",
    "# Add title\n",
    "ax.set_title('Feature Sensitivity to Parameters (Higher = More Informative)', \n",
    "            fontsize=12, fontweight='bold', pad=20)\n",
    "\n",
    "# Add grid\n",
    "ax.set_xticks(np.arange(len(FEATURE_NAMES)) - 0.5, minor=True)\n",
    "ax.set_yticks(np.arange(len(theta_labels)) - 0.5, minor=True)\n",
    "ax.grid(which='minor', color='white', linestyle='-', linewidth=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print top features for each parameter\n",
    "print(\"\\nTop 5 Most Sensitive Features per Parameter:\")\n",
    "print(\"=\"*70)\n",
    "for param_idx, param_x in enumerate(theta_labels):\n",
    "    top_indices = np.argsort(sensitivity_matrix[param_idx])[-5:][::-1]\n",
    "    print(f\"\\n{param_x}:\")\n",
    "    for rank, idx in enumerate(top_indices, 1):\n",
    "        sens = sensitivity_matrix[param_idx, idx]\n",
    "        print(f\"  {rank}. {FEATURE_NAMES[idx]:30s} (sensitivity: {sens:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 5: Pairwise Feature Relationships\n",
    "\n",
    "Examine correlations between key features to understand redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Visualization 5: Feature Correlation Analysis ===\n",
    "\n",
    "# Collect all feature values across all simulations\n",
    "all_features = []\n",
    "\n",
    "for param_x in theta_labels:\n",
    "    for key in results_mean[param_x].keys():\n",
    "        # Get all x-values for this gradient level\n",
    "        all_features.append(results_mean[param_x][key])\n",
    "\n",
    "all_features = np.vstack(all_features)\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = np.corrcoef(all_features.T)\n",
    "\n",
    "# Plot correlation heatmap\n",
    "fig, ax = plt.subplots(figsize=(18, 16))\n",
    "\n",
    "im = ax.imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1, aspect='auto')\n",
    "\n",
    "# Set ticks\n",
    "ax.set_xticks(np.arange(len(FEATURE_NAMES)))\n",
    "ax.set_yticks(np.arange(len(FEATURE_NAMES)))\n",
    "ax.set_xticklabels(FEATURE_NAMES, rotation=90, fontsize=9)\n",
    "ax.set_yticklabels(FEATURE_NAMES, fontsize=9)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label('Correlation', rotation=270, labelpad=20)\n",
    "\n",
    "# Add title\n",
    "ax.set_title('Feature Correlation Matrix (All Parameters)', \n",
    "            fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# Add grid lines at group boundaries\n",
    "group_boundaries = [0]\n",
    "for group_indices in FEATURE_GROUPS.values():\n",
    "    group_boundaries.append(group_boundaries[-1] + len(group_indices))\n",
    "\n",
    "for boundary in group_boundaries[1:-1]:\n",
    "    ax.axhline(boundary - 0.5, color='black', linewidth=2)\n",
    "    ax.axvline(boundary - 0.5, color='black', linewidth=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find highly correlated feature pairs (|corr| > 0.9)\n",
    "high_corr_pairs = []\n",
    "for i in range(len(FEATURE_NAMES)):\n",
    "    for j in range(i+1, len(FEATURE_NAMES)):\n",
    "        if abs(corr_matrix[i, j]) > 0.9:\n",
    "            high_corr_pairs.append((FEATURE_NAMES[i], FEATURE_NAMES[j], corr_matrix[i, j]))\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(\"\\nHighly Correlated Features (|r| > 0.9):\")\n",
    "    print(\"=\"*70)\n",
    "    for feat1, feat2, corr in sorted(high_corr_pairs, key=lambda x: abs(x[2]), reverse=True):\n",
    "        print(f\"  {feat1:30s} <-> {feat2:30s}: r={corr:+.3f}\")\n",
    "    print(\"\\nNote: Highly correlated features may be redundant for inference.\")\n",
    "else:\n",
    "    print(\"\\n✓ No highly correlated features (|r| > 0.9)\")\n",
    "    print(\"  This is good - features are relatively independent!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbi_jax_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
